[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual-Analytics-Applications",
    "section": "",
    "text": "This is a documentation of my full Visual Analytics and Application learning journey."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Ex01/Ex01.html",
    "href": "Hands-on_Ex/Ex01/Ex01.html",
    "title": "Hands-on Ex01: Creating Elegant Graphics with ggplot2",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\npacman::p_load(tidyverse)\n\nThis p_load() function includes the function of running below commands. More effective nowadays vs the old way below. The pacman in front is to make sure even if the package pacman is not installed.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Ex01/Ex01.html#using-r-graphics",
    "href": "Hands-on_Ex/Ex01/Ex01.html#using-r-graphics",
    "title": "Hands-on Ex01: Creating Elegant Graphics with ggplot2",
    "section": "3.1 Using R Graphics",
    "text": "3.1 Using R Graphics\n\nhist(exam_data$MATHS)"
  },
  {
    "objectID": "Hands-on_Ex/Ex01/Ex01.html#using-ggplot2",
    "href": "Hands-on_Ex/Ex01/Ex01.html#using-ggplot2",
    "title": "Hands-on Ex01: Creating Elegant Graphics with ggplot2",
    "section": "3.2 Using ggplot2",
    "text": "3.2 Using ggplot2\n\nggplot(data = exam_data,\n       aes(x = MATHS)) +\n  geom_histogram(bins = 20,\n                 boundary = 100,\n                 color = 'green',\n                 fill = 'navy') +\n  ggtitle(\"Distribution of Maths scores\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex01/Ex01.html#using-ggplot2-and-group-by-genders",
    "href": "Hands-on_Ex/Ex01/Ex01.html#using-ggplot2-and-group-by-genders",
    "title": "Hands-on Ex01: Creating Elegant Graphics with ggplot2",
    "section": "3.3 Using ggplot2 and group by genders",
    "text": "3.3 Using ggplot2 and group by genders\n\nggplot(data = exam_data,\n       aes(x = MATHS,\n           fill = GENDER)) +\n  geom_histogram(bins = 20,\n                 boundary = 100,\n                 color = 'grey30')"
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html",
    "href": "Hands-on_Ex/Ex02/Ex02.html",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "",
    "text": "This exercise looks into the usage of tidyverse package mainly."
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html#library-installation-and-launching",
    "href": "Hands-on_Ex/Ex02/Ex02.html#library-installation-and-launching",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "1 Library installation and launching",
    "text": "1 Library installation and launching\nThe code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are, then they will be launched into R.\n\nlibrary(pacman)\n\npacman::p_load(tidyverse, \n               ggrepel)"
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html#data-loading",
    "href": "Hands-on_Ex/Ex02/Ex02.html#data-loading",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "2 Data Loading",
    "text": "2 Data Loading\n\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html#ggplot2-annotation",
    "href": "Hands-on_Ex/Ex02/Ex02.html#ggplot2-annotation",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "3 ggplot2 Annotation",
    "text": "3 ggplot2 Annotation\nTo begin with, we use ggplot2 to plot some basic graphs first.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNoticed on the overly crowded labels. To avoid text labels from being overlapping with each other, ggrepel is preferred here instead.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: ggrepel: 317 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html#ggplot2-themes",
    "href": "Hands-on_Ex/Ex02/Ex02.html#ggplot2-themes",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "4 ggplot2 Themes",
    "text": "4 ggplot2 Themes\n\n4.1 Eight built-in themes\ntheme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_light() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n4.2 ggtheme package\n\nlibrary(ggthemes)\n\nggthemes provides ‘ggplot2’ themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, ‘Stata’, ‘Excel’, and The Wall Street Journal, among others.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_economist() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_excel() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_few() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_fivethirtyeight() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_solarized() +  \n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n4.3 hrbrthemes package\n\nlibrary(hrbrthemes)\n\nNOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes.\n\n\n      Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and\n\n\n      if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow\n\n\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_ipsum_ps(axis_title_size = 18,\n                 base_size = 15,\n                 grid = \"Y\") +  \n  ggtitle(\"Distribution of Maths scores\") \n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family not\nfound in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family not found in Windows font database\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily not found in Windows font database"
  },
  {
    "objectID": "Hands-on_Ex/Ex02/Ex02.html#composite-plot",
    "href": "Hands-on_Ex/Ex02/Ex02.html#composite-plot",
    "title": "Hands-on Ex02: Beyond ggplot2 Fundamentals",
    "section": "5 Composite plot",
    "text": "5 Composite plot\nTo create multiple statistical graphs first.\n\np1 <- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np2 <- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np3 <- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\nThere are several ggplot2 extensions provide functions to compose figure with multiple graphs.\nPatchwork is one of packages that has a very simple syntax where we can create layouts super easily.\nHere’s the general syntax that combines: - Two-Column Layout using the Plus Sign +. - Parenthesis () to create a subplot group. - Two-Row Layout using the Division Sign\n\nlibrary(patchwork)\n\n\np1 + p2 / p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n(p1 / p2) | p3\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMore on how to stack or pack differently using Patchwork.\n\n5.1 Auto-tagging\npatchwork provides auto-tagging capabilities, in order to identify subplots in text:\n\n((p1 / p2) | p3) +\n  plot_annotation(tag_levels = 'I')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n5.2 Apply themes to all subplots\n\npatchwork <- (p1 / p2) | p3\npatchwork & theme_economist()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n5.3 Place plots or graphic elements on top or below another plot\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html",
    "href": "Hands-on_Ex/Ex03/Ex03.html",
    "title": "Hands-on Ex03",
    "section": "",
    "text": "pacman::p_load(ggiraph,\n               plotly,\n               gganimate,\n               DT,\n               tidyverse,\n               patchwork)\n\nData to be used in this exercise remains the same as the exam data.\n\nexam_data <- read_csv(\"Data/Exam_data.csv\")\n\n\n\n\nTooltip’s interactivity: By hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\nBased on hands-on exercise 1, usually this is how a dot plot looks like. And this is a static plot.\n\np_old <- ggplot(data=exam_data,\n                aes(x = MATHS)) +\n  geom_dotplot(binwidth = 2.5, \n               dotsize = 0.5) +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\n\np_old\n\n\n\n\nBy using ggirafe, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nRemark: svg: Scalable vector graphics, the graph would scale accordingly on different devices to preserve the image quality.\n\n\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below. To present more information inside tooltip, a new data column could be created in order to store the information needed.\n\nexam_data$tooltip <- c(paste0(\n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nAnd we can customize tooltip aesthetic style by adding css declarations using ops_tooltip().\nBelow, the background color of the tooltip and the font color has been changed.\n\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)                                        \n\n\n\n\n\n\n\n\nStatistics can be displayed in the tooltip as an advanced way to customise tooltips. A function is built below to compute 90% confidence interval of the mean.\n\ntooltip <- function(y, ymax, accuracy = .01) { \n  mean <- scales::number(y, accuracy = accuracy)  \n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(\n                     tooltip(y, ymax))),\n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,\n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, linewidth = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\nAs an instance, elements associated with defined data_id (CLASS) will be highlighted upon mouse moving over. This is the second interactive feature of ggiraph.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             #<<\n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\n\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, #<<\n        data_id = CLASS),#<<              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\n\n\n\nWeb document link with a data object will be displayed on the web browser upon mouse click.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              #<<\n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                                        \n\n\n\n\n\n\n\n\nWith this coordinated, when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nTo do that, two steps could be applied:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\nPatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np1 <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + #<<\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\n\np2 <- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + #<<\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 / p2), #<<\n       width_svg = 6,\n       height_svg = 6,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nTake note that the data_id aesthetic is critical to link observations between plots. The tooltip aesthetic is optional but nice to have when hovering the mouse over a point."
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#interactive-scatter-plot-using-plot_ly-method",
    "href": "Hands-on_Ex/Ex03/Ex03.html#interactive-scatter-plot-using-plot_ly-method",
    "title": "Hands-on Ex03",
    "section": "2.1 Interactive scatter plot using plot_ly() method",
    "text": "2.1 Interactive scatter plot using plot_ly() method\n\nplot_ly(data = exam_data, \n        x = ~MATHS, \n        y = ~ENGLISH,\n        color = ~RACE,\n        colors = \"Set1\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#customising-tooltip",
    "href": "Hands-on_Ex/Ex03/Ex03.html#customising-tooltip",
    "title": "Hands-on Ex03",
    "section": "2.2 Customising tooltip",
    "text": "2.2 Customising tooltip\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS,\n        text = ~paste(\"Student ID:\", ID,     \n                      \"<br>Class:\", CLASS),  \n        color = ~RACE, \n        colors = \"Set1\") %>%\n  layout(title = 'English Score versus Maths Score ', #<<\n         xaxis = list(range = c(0, 100)),             #<<\n         yaxis = list(range = c(0, 100)))             #<<"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#coordinated-multiple-views-with-plotly",
    "href": "Hands-on_Ex/Ex03/Ex03.html#coordinated-multiple-views-with-plotly",
    "title": "Hands-on Ex03",
    "section": "2.3 Coordinated multiple views with plotly",
    "text": "2.3 Coordinated multiple views with plotly\n\n2.3.1 Using subplot() & highlight_key() from plotly package\n\nd <- highlight_key(exam_data)  #<<\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#interactive-data-table-dt-package",
    "href": "Hands-on_Ex/Ex03/Ex03.html#interactive-data-table-dt-package",
    "title": "Hands-on Ex03",
    "section": "3.1 Interactive data table: DT package",
    "text": "3.1 Interactive data table: DT package\n\nDT::datatable(exam_data, class= \"compact\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#linked-brushing",
    "href": "Hands-on_Ex/Ex03/Ex03.html#linked-brushing",
    "title": "Hands-on Ex03",
    "section": "3.2 Linked brushing",
    "text": "3.2 Linked brushing\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document.\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Ex03/Ex03.html#packages-to-be-used",
    "href": "Hands-on_Ex/Ex03/Ex03.html#packages-to-be-used",
    "title": "Hands-on Ex03",
    "section": "4.1 Packages to be used",
    "text": "4.1 Packages to be used\n\ngganimate: An ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. Here We only want to use its country_colors scheme.\n\n\npacman::p_load(readxl,\n               gganimate,\n               gifski,\n               gapminder)\n\n\ncol <- c(\"Country\", \"Continent\")\nglobalPop <- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %>%\n  mutate_each_(funs(factor(.)), col) %>%\n  mutate(Year = as.integer(Year))\n\nFirstly, a static population bublle plot is built using ggplot2.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\nNow, we enable the transition using gganimate. Noted that,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) + \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html",
    "href": "Hands-on_Ex/Ex04/Ex04.html",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "",
    "text": "Packages that will be focused on for this exercise is mainly ggstatsplot.\n\npacman::p_load(ggstatsplot, \n               tidyverse)\n\n\nexam <- read_csv(\"Data/Exam_data.csv\")\n\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\nTo build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\nggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\nA visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\nexam1 <- exam %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nggbarstats(exam1,\n           x = MATHS_bins,\n           y = GENDER)\n\n\n\n\n\n\n\nThe purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\npacman::p_load(readxl, \n               performance, \n               parameters, \n               see)\n\n\ncar_resale <- read_xls(\"Data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model       Price Age_0…¹ Mfg_M…² Mfg_Y…³     KM Quart…⁴ Weight Guara…⁵\n   <dbl> <chr>       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>\n 1    81 TOYOTA Cor… 18950      25       8    2002  20019     100   1180       3\n 2     1 TOYOTA Cor… 13500      23      10    2002  46986     210   1165       3\n 3     2 TOYOTA Cor… 13750      23      10    2002  72937     210   1165       3\n 4     3  TOYOTA Co… 13950      24       9    2002  41711     210   1165       3\n 5     4 TOYOTA Cor… 14950      26       7    2002  48000     210   1165       3\n 6     5 TOYOTA Cor… 13750      30       3    2002  38500     210   1170       3\n 7     6 TOYOTA Cor… 12950      32       1    2002  61000     210   1170       3\n 8     7  TOYOTA Co… 16900      27       6    2002  94612     210   1245       3\n 9     8 TOYOTA Cor… 18600      30       3    2002  75889     210   1245       3\n10    44 TOYOTA Cor… 16950      27       6    2002 110404     234   1255       3\n# … with 1,426 more rows, 28 more variables: HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>,\n#   Radio <dbl>, Mistlamps <dbl>, Sport_Model <dbl>, Backseat_Divider <dbl>, …\n\n\nTo calibrate a multiple linear regression model using lm() from base stats of R.\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Guarantee_Period  1.04   [1.01, 1.17]         1.02      0.97     [0.86, 0.99]\n        Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n         Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\nHigh Correlation\n\n   Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n     KM 1.46 [1.37, 1.57]         1.21      0.68     [0.64, 0.73]\n Weight 1.41 [1.32, 1.51]         1.19      0.71     [0.66, 0.76]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n <- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\n\ncheck_h <- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nplot() from see package\nparameters() from parameters package\n\n\nplot(parameters(model1))\n\n\n\n\n\nggcoefstats() from ggstatplot package\n\n\nggcoefstats(model1,\n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-ggplot2-methods",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-ggplot2-methods",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "2.1 Using ggplot2 methods",
    "text": "2.1 Using ggplot2 methods\n\npacman::p_load(tidyverse, \n               plotly, \n               crosstalk, \n               DT, \n               ggdist, \n               gganimate)\n\nA point estimate is a single number, such as a mean. Uncertainty is expressed as standard error, confidence interval, or credible interval\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\nTo reveal the standard error of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by rac\")\n\n\n\n\nTo plot out the 95% confidence interval:\nThe error bars is sorted by the average maths score."
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-ggdist",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-ggdist",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "2.2 Using ggdist",
    "text": "2.2 Using ggdist\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +   #<<\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nTo make use of some arguments customization.\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nTo make use of stat_gradientinterval() from ggdist.\n\n# exam %>%\n#   ggplot(aes(x = RACE, \n#              y = MATHS)) +\n#   stat_gradientinterval(   \n#     fill = \"skyblue\",      \n#     show.legend = TRUE     \n#   ) +                        \n#   labs(\n#     title = \"Visualising confidence intervals of mean math score\",\n#     subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-ungeviz-package-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-ungeviz-package-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "2.3 Using ungeviz package with Hypothetical Outcome Plots (HOPs)",
    "text": "2.3 Using ungeviz package with Hypothetical Outcome Plots (HOPs)\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE),\n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    # size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-funnelplotr-for-the-basic-plot",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-funnelplotr-for-the-basic-plot",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "3.1 Using FunnelPlotR for the basic plot",
    "text": "3.1 Using FunnelPlotR for the basic plot\nThe very basic version without any make-up looks like below.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",\n  xrange = c(0, 6500),\n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\",           \n  x_label = \"Cumulative COVID-19 Positive Cases\",\n  y_label = \"Cumulative Fatality Rate\"\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. Some of the key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc."
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-ggplot2-to-customise-funnel-plot",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-ggplot2-to-customise-funnel-plot",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "3.2 Using ggplot2 to customise funnel plot",
    "text": "3.2 Using ggplot2 to customise funnel plot\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\n# To compute the fit.mean\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n# To compute the lower and upper limits for 95% confidence interval\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, number.ul999, number.seq, fit.mean)\n\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np"
  },
  {
    "objectID": "Hands-on_Ex/Ex04/Ex04.html#using-plotly-ggplot2-to-make-it-interactive",
    "href": "Hands-on_Ex/Ex04/Ex04.html#using-plotly-ggplot2-to-make-it-interactive",
    "title": "Hands-on Ex04: Fundamentals of Visual Analytics",
    "section": "3.3 Using plotly & ggplot2 to make it interactive",
    "text": "3.3 Using plotly & ggplot2 to make it interactive\n\nfp_ggplotly <- ggplotly(p,\n                      tooltip = c(\"label\",\n                                  \"x\",\n                                  \"y\"))\n\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Ex05/Ex05.html",
    "href": "Hands-on_Ex/Ex05/Ex05.html",
    "title": "Hands-on Ex05: Visual Multivariate Analysis",
    "section": "",
    "text": "pacman::p_load(corrplot,\n               tidyverse,\n               ggstatsplot,\n               ggcorrplot,\n               plotly,\n               seriation,\n               dendextend,\n               heatmaply,\n               GGally,\n               parallelPlot)\n\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\nwine <- read.csv(\"Data/wine_quality.csv\")\n\nBeside quality and type, the rest of the variables are numerical and continuous data type.\n\n\n\npairs(wine[, 1:11],\n      upper.panel = NULL)\n\n\n\n\n\n\n\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor <- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr <- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr <- abs(cor(x, y, use=\"complete.obs\"))\ntxt <- format(c(r, 0.123456789), digits=digits)[1]\ntxt <- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations).\nCorrgram data visualisation technique is able to overcome this problem, .\n\nggstatsplot::ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\",\n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p < 0.05\"\n)\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p < 0.05\"\n) \n\n\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\nTake note the sub-code chunk above can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\n\n\nFaceting is not available in ggcorrmat() but can be found in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\nTake note that:\n\nTo build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package.\n\n\n\n\n\nThe default visual object in corrplot used to plot the corrgram is circle.\nThe default layout of the corrgram is a symmetric matrix. We can switch in between “full”, “upper” and “lower” using parameter “type”.\nThe default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\nwine.cor <- cor(wine[, 1:11])\ncorrplot(wine.cor,\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\nSome other parameters that can be changed to customise the layout such as: tl.pos, tl.cex, tl.offset, cl.pos, cl.cex and cl.offset.\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are: AOE, FPC, hclust and alphabet.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex05/Ex05.html#heatmap-of-r-stats",
    "href": "Hands-on_Ex/Ex05/Ex05.html#heatmap-of-r-stats",
    "title": "Hands-on Ex05: Visual Multivariate Analysis",
    "section": "2.1 heatmap() of R stats",
    "text": "2.1 heatmap() of R stats\nThe arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nwh_heatmap <- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)"
  },
  {
    "objectID": "Hands-on_Ex/Ex05/Ex05.html#a-cluster-heatmap",
    "href": "Hands-on_Ex/Ex05/Ex05.html#a-cluster-heatmap",
    "title": "Hands-on Ex05: Visual Multivariate Analysis",
    "section": "2.2 A cluster heatmap",
    "text": "2.2 A cluster heatmap\n\nwh_heatmap <- heatmap(wh_matrix,\n                      scale = 'column',\n                      margins = c(10, 4),\n                      cexRow = 0.6,\n                      cexCol = 0.8)\n\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\nThe Happiness Score variable have relatively higher values, therefore it might make other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs. Margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands-on_Ex/Ex05/Ex05.html#interactive-heatmap-using-heatmaply",
    "href": "Hands-on_Ex/Ex05/Ex05.html#interactive-heatmap-using-heatmaply",
    "title": "Hands-on Ex05: Visual Multivariate Analysis",
    "section": "2.3 Interactive heatmap using heatmaply",
    "text": "2.3 Interactive heatmap using heatmaply\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap. The text label of each raw, on the other hand, is placed on the right hand side of the heat map.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n2.3.1 Data transformation supported by heatmaply()\n\n2.3.1.1 Scaling method\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n2.3.1.2 Normalising method\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations. This preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”. Different from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\n2.3.1.3 Percentising method\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank. This is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile. The benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it. Similar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\n\n2.3.2 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n2.3.2.1 Manual approach\nUsing hierarchical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n2.3.2.2 Statistical approach\nThe best clustering method and number of cluster would be determined using the dend_expend() and find_k() functions of dendextend package.\n\nwh_d <- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust <- hclust(wh_d, method = \"average\")\nnum_k <- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nTherefore,\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n2.3.2.3 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nOr we can choose “GW” instead of “OLO”. GW aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option of ‘mean’ would give the output that is similar to what we would get from heatmap functions in other packages like gplots::heatmap.2\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” would give the dendrograms without any rotations that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n2.3.3 Color palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n2.3.4 Other cartographic advantages from heatmaply\nIn the code chunk below, some of following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "By the end of this hands-on exercise I will create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#getting-started",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#getting-started",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nTo check, install and launch the following R packages:\n\n\nShow the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "3 Plotting Calendar Heatmap",
    "text": "3 Plotting Calendar Heatmap\nBy the end of this section, I will:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n3.1 The Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n3.2 Importing the data\nTo import eventlog.csv file into R environment. The file is called the data frame as attacks.\n\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n3.3 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\n\ntz field stores time zone of the source IP address.\n\n\n\n3.4 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, I will write a function to perform the task.\n\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nNote: ymd_hms() and hour() are from lubridate package and weekdays() is a base R function.\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nNote: Beside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n3.5 Building the Calendar Heatmaps\n\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\nThings learnt from the code chunk: - a tibble data table called grouped is derived by aggregating the attack by wkday and hour fields. - a new field called n is derived by using group_by() and count() functions. - na.omit() is used to exclude missing value. - geom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles. - theme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot. - coord_equal() is used to ensure the plot will have an aspect ratio of 1:1. - scale_fill_gradient() function is used to creates a two colour gradient (low-high).\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n3.6 Building Multiple Calendar Heatmaps\nThe problem that I am trying to tackle is to build multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\n3.7 Plotting Multiple Calendar Heatmaps\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, the followings steps should be considered:\n\ncount the number of attacks by country,\ncalculate the percent of attacks by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, I will extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()\n\n\n\n3.8 Plotting Multiple Calendar Heatmaps\nStep 3: I will plot the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#cycle-plot",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#cycle-plot",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "4 Cycle Plot",
    "text": "4 Cycle Plot\nIn this section, a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions will be done.\n\n\n4.1 Data Preparation\n\n4.1.1 Step 1: Data Import\nFor the purpose of this exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n4.1.2 Step 2: Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\n\n4.1.3 Step 3: Extracting the target country\nNext, the code chunk below is to extract data for the target country (i.e. Vietnam)\n\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\n\n4.1.4 Step 4: Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n4.2 Step 5: Plotting the cycle plot\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             linewidth=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "5 Plotting Slopegraph",
    "text": "5 Plotting Slopegraph\nCGPfunctions needs to be installed and loaded onto R environment. The newggslopegraph function would be the main focus of this section.\n\npacman::p_load(CGPfunctions)\n\n\n5.0.1 Step 1: Data Import\n\nrice <- read_csv(\"data/rice.csv\")\n\n\n\n5.0.2 Step 2: Slopegraph plotting\nA basic slopegraph can be plotted like this below:\n\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Times = Year, \n                  Measurement = Yield, \n                  Grouping = Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Li Ziyi\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-horizon-plot",
    "href": "Hands-on_Ex/Ex06/Hands-on_Ex06-VisTime.html#plotting-horizon-plot",
    "title": "Hands-on Exercise 6: Visualising and Analysing Time-oriented Data",
    "section": "6 Plotting Horizon Plot",
    "text": "6 Plotting Horizon Plot\nggHoriPlot is the package to be used for horizon plot mainly.\n\npacman::p_load(ggHoriPlot)\n\n\n6.0.1 Step 1: Data Import\nFor the purpose of this exercise, Average Retail Prices Of Selected Consumer Items will be used.\n\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))\n\nTake note that dmy() from lubridate package allows us to palse the Date field into appropriate Date data type in R.\n\n\n6.0.2 Step 2: Plotting the horizon graph\n\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html",
    "title": "Hands-on_Ex07",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, I will plot functional and truthful choropleth maps by using an R package called tmap package.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\nThe first data set to be used for below is called SGPools_svy21. The data is in csv file format.\nFor the second part of this exercise, two data sets will be used below to create the choropleth map:\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg. This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nmpsz <- st_read(dsn = \"Data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `G:\\My Drive\\SMU MITB\\ISSS608 Visual Analytics and Applications\\lzy66666666\\Visual-Analytics-Applications\\Hands-on_Ex\\Ex07\\Data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\npopdata <- read_csv(\"Data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nTake note that:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nTake note that:\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section later.\nThe default colour scheme used is YlOrRd of ColorBrewer. By default, Missing value will be shaded in grey.\n\n\n\n\ntm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nTake note that:\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\nA quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nAn equal data classification method\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTake note that from above plots, the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore getting started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe may reverse the color shading by adding a prefix of “-” in front of the color.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nE.g. classic style\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#data-wangling",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#data-wangling",
    "title": "Hands-on_Ex07",
    "section": "2.1 Data Wangling",
    "text": "2.1 Data Wangling\nThe data set used for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches.\n\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME                            ADDRESS POSTC…¹ XCOORD YCOORD OUTLE…² Gp1Gp…³\n   <chr>                           <chr>     <dbl>  <dbl>  <dbl> <chr>     <dbl>\n 1 Livewire (Marina Bay Sands)     2 Bayf…   18972 30842. 29599. Branch        5\n 2 Livewire (Resorts World Sentos… 26 Sen…   98138 26704. 26526. Branch       11\n 3 SportsBuzz (Kranji)             Lotus …  738078 20118. 44888. Branch        0\n 4 SportsBuzz (PoMo)               1 Sele…  188306 29777. 31382. Branch       44\n 5 Prime Serangoon North           Blk 54…  552542 32239. 39519. Branch        0\n 6 Singapore Pools Woodlands Cent… 1A Woo…  731001 21012. 46987. Branch        3\n 7 Singapore Pools 64 Circuit Rd … Blk 64…  370064 33990. 34356. Branch       17\n 8 Singapore Pools 88 Circuit Rd … Blk 88…  370088 33847. 33976. Branch       16\n 9 Singapore Pools Anchorvale Rd … Blk 30…  540308 33910. 41275. Branch       21\n10 Singapore Pools Ang Mo Kio N2 … Blk 20…  560202 29246. 38943. Branch       25\n# … with 296 more rows, and abbreviated variable names ¹​POSTCODE,\n#   ²​`OUTLET TYPE`, ³​`Gp1Gp2 Winnings`\n\n\nWe convert sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf <- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io."
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#drawing-proportional-symbol-map",
    "title": "Hands-on_Ex07",
    "section": "2.2 Drawing Proportional Symbol Map",
    "text": "2.2 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\n\ntmap_mode(\"view\")\n\n\ntm_shape(sgpools_sf) + \ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n2.2.1 Attributes with different colors\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n2.2.2 tm_facets() to produce multiple maps with synchronised zoom and pan settings\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#packages-and-data-loading",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#packages-and-data-loading",
    "title": "Hands-on_Ex07",
    "section": "3.1 Packages and data loading",
    "text": "3.1 Packages and data loading\n\npacman::p_load(tmap, tidyverse, sf)\n\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#visualising-distribution-of-non-functional-water-point",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#visualising-distribution-of-non-functional-water-point",
    "title": "Hands-on_Ex07",
    "section": "3.2 Visualising distribution of non-functional water point",
    "text": "3.2 Visualising distribution of non-functional water point\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#choropleth-map-for-rates",
    "title": "Hands-on_Ex07",
    "section": "3.3 Choropleth Map for Rates",
    "text": "3.3 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n3.3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n3.3.2 Plotting map of rate\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "href": "Hands-on_Ex/Ex07/Hands-on_Ex07.html#extreme-value-maps",
    "title": "Hands-on_Ex07",
    "section": "3.4 Extreme value maps",
    "text": "3.4 Extreme value maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n3.4.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\nCreating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\nreturns: v: vector with values (without a column name)\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n3.4.1.1 A percentile mapping function\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\npercentmap(\"total_wp\",\n           NGA_wp)\n\n\n\n\n\n\n\n\n\n3.4.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\n\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Take-home_Ex/Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "In this exercise, the basic demographic structure of Singapore top 9 biggest planning areas categorised by gender and age group is being studied.\nData used is a csv file extracted from Department of Statistics, Singapore. The population data snapshot from June 2022 is being studied mainly."
  },
  {
    "objectID": "Take-home_Ex/Ex01/Take-home_Ex01.html#data-preparation",
    "href": "Take-home_Ex/Ex01/Take-home_Ex01.html#data-preparation",
    "title": "Take-home Exercise 01",
    "section": "2 Data Preparation",
    "text": "2 Data Preparation\n\n\n\n\n2.1 Data Cleaning\nPreview the downloaded csv file, the data looks generally clean with no “Total” elements inside all columns. This is specifically checked to ensure there won’t be double counting from aggregation blended inside. Inside Tableau, if needed, total could be easily computed by summing up all the rest data.\n\n\n\n\n\nRaw data used\n\n\n\n\n\n\n2.2 Data Loading\nOpen a new Tableau workbook and import data using the csv file prepared above.\nFor easier data manipulation later on, some columns were renamed for easier understanding and convenient usage, e.g. ‘AG’ to ‘Age Group’, ‘Pop’ to ‘Population’ etc.\n\n\n\n\n\nTableau data loading"
  },
  {
    "objectID": "Take-home_Ex/Ex01/Take-home_Ex01.html#data-visualisation",
    "href": "Take-home_Ex/Ex01/Take-home_Ex01.html#data-visualisation",
    "title": "Take-home Exercise 01",
    "section": "3 Data Visualisation",
    "text": "3 Data Visualisation\nAge-sex pyramid was taught be an analytical visualisation commonly used by demographers to reveal the structure of population by gender and age group.\n\n3.1 Overall Singaporean Population Pyramid\nTwo calculated fields are created to facilitate the visualisation preparation:\n    Male:\n    if [Sex]=\"Males\"\n    then [Population]\n    end\n    \n    Female:\n    if [Sex]=\"Females\"\n    then [Population]\n    end\nTo produce the desired visualisation, follow below steps:\n\nDrag the ‘Age Group’ from Data panel into Rows bar.\nSort the ‘Age Group’ by clicking the white downwards pointing arrow, choose ‘Sort’ and sort by ‘Alphabetic’ in descending order.\nDrag the line item ‘5_to_9’ inside the worksheet from ‘Age Group’ column to one line above ‘0_to_4’. With this, the age group should have been sorted in a descending manner entirely.\nDrag the ‘Population’ from Data panel into Columns bar twice. Two ‘SUM(Population)’ should be seen in Columns’ bar. This step is to facilitate the pyramid making. Whereby the left side will be intended to use for female population visualisation and the right side will be intended to use to male population visualisation.\nRight click the axis of the ‘Population’ on the left and edit axis. Choose the option of ‘Reversed’ under ‘Scale’. This will reverse the axis on the left. Rename the axis on the left to ‘Female’.\nRight click the axis on the right. Edit the axis and rename it to ‘Male’.\n\nThere is a pyramid shape built now. Proceeding next to isolate each gender’s population from the other.\n\nCreate two newly calculated measures named ‘Male Color’ and ‘Female Color’. This is to facilitate the visualisation of pyramid later.\nMale Color: \nstr(if [Sex]=\"Males\" \nthen 1 \nelse 0 \nend)\n\nFemale Color: \nstr(if [Sex]=\"Females\" \nthen 1 \nelse 0 \nend)\nDrag the ‘Female Color’ from Data panel into Marks section under the ‘Color’ button from the first ‘SUM(Population)’. Choose a color that you would like to assign to Female population data (1). Here, color pink is used. When the data is not ‘Female’ (0), color grey is used.\nSimilarly, drag the ‘Male Color’ from Data panel and do the same but put it under under the second ‘SUM(Population)’. Choose a color that you would like to assign to Male population data (1). Here, color blue is used. When the data is not ‘Male’ (0), color grey is used to align with the color used in ‘Female Color’.\nFor better analysis usage, under ‘Marks’ section for both ‘SUM(Population)’, ‘Allow labels to overlap other marks’ is chosen under ‘Label’. This will enable us to see all population data on the chart.\nChoose ‘Entire View’ on the top menu to allow bigger chart being presented when in full screen mode.\nRename the tab with an informative title.\n\nTherefore, a population pyramid of Singapore has been completed. See below.\n\n\n\n\n\nPopulation pyramid of Singapore\n\n\n\n\n\n\n3.2 Selection of Nine Planning Areas\nFor this take-home exercise, nine planning areas are supposed to be chosen and presented on a single view.\nTo find out which nine areas to work on, it’s more of my interest to filter based on each of their total population. Below actions are taken in order to achieve that.\n\nCreate a new worksheet.\nDrag the ‘Population’ values from Data panel into Columns bar.\nDrag the ‘Planning Area’ text from Data panel into Rows bar.\nSort the ‘Planning Area’ by clicking the white downwards pointing arrow, choose ‘Sort’ and sort by ‘Field’ in descending order. Choose the field to be sorted by ‘Population’ from the drop-down list. The aggregation we use here remains ‘Sum’.\n\nThis would enable us to visualise the top planning areas according to each of their total area population. To make the list of top area more clearly, parameters and additional calculated fields are created.\nCreation of parameter ‘Top Planning Areas’.\n\n\n\n\n\nParameter: Top planning area\n\n\n\n\nCreation of set ‘Top N Planning Area’ derived from item ‘Planning Area’ from the Data panel by right clicking it:\n\n\n\n\n\nSet: Top N Planning Area\n\n\n\n\nCreation of calculated field ‘Labels’. This label is to be displayed inside the table later for easier readability.\n    if [Top N Planning Area]\n    then \"Top \" + str([Top Planning Area]) + \" Planning Areas\"\n    else \"Others\"\n    end\nWith these, below chart is completed.\n\n\n\n\n\nTop 9 planning area by total population\n\n\n\n\nFrom here, the nine areas that are chosen to study on are:\n\nBedok,\nTampines,\nJurong West,\nSengkang,\nWoodlands,\nHougang,\nYishun,\nChoa Chu Kang,\nPunggol.\n\n\n\n3.3 Trellis Display of Population Pyramid of The Nine Planning Areas\nIn this exercise, the nine planning areas are requested to be presented on a single view. To do that, a trellis display will be applied. Following below steps that will lead us to the desired output.\n\nDuplicate the worksheet ‘PA comparison of Female and Male population in Singapore broken down by age group 2022’.\nRename the worksheet to ‘Trellis comparison of Female and Male population in Top 9 Population Areas of Singapore broken down by age group 2022’.\nDrag the ‘Planning Area’ text from Data panel into Columns bar.\nDrag the ‘Planning Area’ text from Data panel into Filters section and tick the chosen nine areas only.\n\nThe trellis display is therefore completed.\n\n\n\n\n\nTop 9 planning area population pyramid\n\n\n\n\n\n\n3.4 Final Dashboard\nThe final dashboard is produced by putting together the three important worksheets. Here, three worksheets have been chosen to present together in one view to complete the whole story.\n\n\n\n\n\nTop 9 planning area population pyramid"
  },
  {
    "objectID": "Take-home_Ex/Ex01/Take-home_Ex01.html#observation-and-insights",
    "href": "Take-home_Ex/Ex01/Take-home_Ex01.html#observation-and-insights",
    "title": "Take-home Exercise 01",
    "section": "4 Observation and Insights",
    "text": "4 Observation and Insights\nThis part is the write-up requested. Words count: 447 words.\n\n4.1 “Constrictive” pyramid (or “negative growth” pyramid)\nThe population pyramid of Singapore observed from the figure above is narrowed at the bottom younger ages, typical for many developed countries in the world. Usually, country of this population pyramid type tends to have a lower birth rate and a lower death rate thanks to prevalent high level of education and accessible health care services.\nThis observation is in line with many incentive measures from the government seen, for example, policies like birth bonuses, child benefits and even tax credits.\n\n\n4.2 The population pyramid of Singapore from 2022 remains like the one from 2017.\nFrom this link shared by prof, the population pyramid of Singapore in 2022 still look very alike in how it was in 2017, despite of much effort from government on improving fertility rate.\nThis is indeed a concerning topic. Such constrictive pyramid indicates a higher dependency ratio (a ratio related to the number of children and older persons to the working-age population), which implies potential work force shortage in the future. Consequentely, that might result in seriously negative economic impact like reduction in taxation and heavier burden on health care industry.\n\n\n4.3 Females outnumber males in Singapore Based on June 2022 data.\nThis trend is observed in majority age groups. Zooming into each group, only for age 19 and below, number of males is observed to be higher. This trend is observed in majority age groups. Zooming into each group, only for age 19 and below, number of males is observed to be higher. The trend also applies to the nine areas that we have chosen to study on. Without more than sufficient research, there are some unproved personal assumptions that might lead to this result:\n\nMales tend to have a higher mortality rate especially at an older age group, mainly due to higher probability of possessing an unhealthy life habit like smoking or heavy drinking.\nA possible relatively higher rate of approval for immigration applications from foreign young females. This might account for some partial efforts from government on improving the fertility rate of Singapore.\n\n\n\n4.4 Punggol and Sengkang each has slightly different population pyramid from the rest of chosen areas’.\nThanks to the Trellis plot, a comparison between the nine areas looks much easier. From the dashboard, Punggol and Sengkang seem to have a relatively larger base (newly born babies) and middle part (working adults) than the rest. This observation tends to prove the point of view that these two areas tend to have higher percentages of young working couples and babies. Hence, Punggol has recently been dubbed as Singapore’s baby town, according to a news report from The Straits Times."
  },
  {
    "objectID": "Take-home_Ex/Ex01/Take-home_Ex01.html#reflection",
    "href": "Take-home_Ex/Ex01/Take-home_Ex01.html#reflection",
    "title": "Take-home Exercise 01",
    "section": "5 Reflection",
    "text": "5 Reflection\nThis is my first attempt on building and publishing . Along the journey, I have been learning from many of my mistakes. E.g. only after building the whole dashboard, I realised there was better data set for me to use and work on (csv preferred over xlsx, if both available). And the very first draft of dashboard looked decent but was only as good as a graph that can be viewed but cannot be analyzed with. Countless attempts of rework and revised versions were created, with much practice time invested.\nNevertheless, I am excited of this continuous learning journey ahead. I look forward to improving my analytics skills using Tableau and R to further improve this exercise with more depth."
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The requirement of this exercise is to critic the submission that revealed demographic structure of Singapore at nine planning areas from one of my classmates in terms of clarity and aesthetics, prepare a sketch for the alternative design by using the data visualisation design principles and best practices learned from Lesson 1 and 2. Eventually, the original design will be remade below by using ggplot2 and its extensions together with tidyverse packages from R.\nOriginal dashboard prepared could be retrieved through this link.\nBelow is a snapshot of the dashboard.\n\n\n\n\n\n\n\n\nOriginal Dashboard"
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#clarity",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#clarity",
    "title": "Take-home Exercise 02",
    "section": "2.1 Clarity",
    "text": "2.1 Clarity\n\n2.1.1 Missing a well worded title\nBy looking at the dashboard itself in full screen mode, there are insufficient information on what message this chart is trying to relay.\nAdditionally, nine planning areas’ demographic structure has been shown indeed. The tableau public dashboard, however, is published using the name of ‘Singapore Population distribution by age’. Misunderstanding might be raised as people might wrongly conclude that there are only nine areas in total in Singapore, based on this dashboard.\n\n\n2.1.2 Misalignment in x-axis across nine plots\nFor the chosen nine areas’ plots, some different scales are observed in the x-axis (population count). Misleading visual representation originated from the graph might result in wrong conclusion being made.\n\n\n2.1.3 Weak analytics capabilities\nDespite the pyramid shape of each of the nine areas has been plotted, the dashboard seems to be short of analytics capabilities overall. A meaningful benchmark seems to be missing here. It might be helpful to improve on this through providing reference on how the overall Singapore population pyramid looks like."
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#aesthetics",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#aesthetics",
    "title": "Take-home Exercise 02",
    "section": "2.2 Aesthetics",
    "text": "2.2 Aesthetics\n\n2.2.1 Usage of an unrecommended color\nInside the dashboard, males population is being represented with a color similar to dark khaki. A color like this that is similar to ‘puke yellow’ is discouraged to be used, according to the class discussion during week 2 lecture.\n\n\n2.2.2 Over-packed y-axis ticks\nGiven the granularity of this study, there are many age groups to be presented on the y-axis. However, the y-axis on the current dashboard is too packed with little to no space in between.\n\n\n2.2.3 Repeated appearance of x & y-axis across all plots\nThe same x & y-axis are used across all nine plannings. Due to the complexity from the data, the dense axes seem to be distracting especially for plots inside the second and third rows and columns."
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-source",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-source",
    "title": "Take-home Exercise 02",
    "section": "4.1 Data source",
    "text": "4.1 Data source\nFor this exercise, the data source remained the same as what was used in exercise 01."
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#libraries-to-be-used",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#libraries-to-be-used",
    "title": "Take-home Exercise 02",
    "section": "4.2 Libraries to be used:",
    "text": "4.2 Libraries to be used:\nIn this exercise, main libraries to be used are listed below. pacman is used to check on and ensure the installation of packages.\n\nlibrary(pacman)\n\npacman::p_load(tidyverse,\n               ggplot2,\n               ggrepel,\n               ggthemes,\n               patchwork,\n               dplyr)"
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-loading",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-loading",
    "title": "Take-home Exercise 02",
    "section": "4.3 Data loading:",
    "text": "4.3 Data loading:\nLoad the csv file below for our usage later.\n\npopulation <- read_csv('Data/SG_Population_Jun2022.csv')\n\nRows: 75696 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, FA\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-wrangling",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#data-wrangling",
    "title": "Take-home Exercise 02",
    "section": "4.4 Data wrangling",
    "text": "4.4 Data wrangling\nAs a good practice, the structure of the raw data is inspected to identify if any data manipulation would be beneficial for our usage later on:\n\nstr(population)\n\nspc_tbl_ [75,696 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA  : chr [1:75696] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr [1:75696] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG  : chr [1:75696] \"0_to_4\" \"0_to_4\" \"0_to_4\" \"0_to_4\" ...\n $ Sex : chr [1:75696] \"Males\" \"Males\" \"Males\" \"Males\" ...\n $ FA  : chr [1:75696] \"<= 60\" \">60 to 80\" \">80 to 100\" \">100 to 120\" ...\n $ Pop : num [1:75696] 0 10 20 60 10 0 0 0 20 50 ...\n $ Time: num [1:75696] 2022 2022 2022 2022 2022 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Sex = col_character(),\n  ..   FA = col_character(),\n  ..   Pop = col_double(),\n  ..   Time = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\nFor easier data manipulation and better visualisation effect, the raw data is duplicated below.\n\npop_temp  <- population\n\nTo keep the age groups arranged in a sorted manner, the format of Age Group column (‘AG’) will be transformed from text to factor. This would enable us to perform necessary adjustment in sorting to get the desired output.\n\npop_temp$AG <- as.factor(pop_temp$AG)\nlevels(pop_temp$AG)\n\n [1] \"0_to_4\"      \"10_to_14\"    \"15_to_19\"    \"20_to_24\"    \"25_to_29\"   \n [6] \"30_to_34\"    \"35_to_39\"    \"40_to_44\"    \"45_to_49\"    \"5_to_9\"     \n[11] \"50_to_54\"    \"55_to_59\"    \"60_to_64\"    \"65_to_69\"    \"70_to_74\"   \n[16] \"75_to_79\"    \"80_to_84\"    \"85_to_89\"    \"90_and_over\"\n\n\nAs can be seen above, the age group ‘5_to_9’ is not falling in the right place. To correct this, a manual swap is done below.\n\npop_temp$AG <- factor(pop_temp$AG,levels(pop_temp$AG)[c(1,10,2:9,11:19)])\nlevels(pop_temp$AG)\n\n [1] \"0_to_4\"      \"5_to_9\"      \"10_to_14\"    \"15_to_19\"    \"20_to_24\"   \n [6] \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"    \"45_to_49\"   \n[11] \"50_to_54\"    \"55_to_59\"    \"60_to_64\"    \"65_to_69\"    \"70_to_74\"   \n[16] \"75_to_79\"    \"80_to_84\"    \"85_to_89\"    \"90_and_over\"\n\n\nFor the ease of visualisation the population of the two genders side by side, an extra column is prepared such that the population of females would be recorded as values with negative signs, while the population of males would be in positive signs.\nConsequently, the female population would be on the left-hand-side and the male population would be on the right-hand-side.\n\npop_temp$Pop_abs <- ifelse(pop_temp$Sex == \"Males\",\n                        pop_temp$Pop,\n                        -1*pop_temp$Pop)"
  },
  {
    "objectID": "Take-home_Ex/Ex02/Take-home_Ex02.html#visualisation",
    "href": "Take-home_Ex/Ex02/Take-home_Ex02.html#visualisation",
    "title": "Take-home Exercise 02",
    "section": "4.5 Visualisation",
    "text": "4.5 Visualisation\n\n4.5.1 Population pyramid of Singapore\nggplot is the main package that is used to plot the desired chart.\n\np1 <- \nggplot(data=pop_temp,\n       aes(x=AG,\n           y=Pop_abs,\n           fill = Sex)) +\n  geom_bar(data=subset(pop_temp, Sex == \"Males\"),stat = \"identity\") +\n    geom_bar(data=subset(pop_temp, Sex == \"Females\"),stat = \"identity\") +\n  xlab(\"Age Group\") +\n  ylab(NULL) +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(-200000, 200000, 50000), \n                      labels = paste0(as.character(c(seq(200, 0, -50), seq(5, 200, 50))),\"k\")) + \n  ggtitle(\"Females vs Males population of Singapore by age group 2022\") +\n  labs(caption = \"Demographic information as of Jun 2022\") +\n  scale_fill_discrete(name=\"\") +\n  theme_economist() + \n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 15,\n                                  hjust = 0.5,\n                                  margin = margin(0,0,10,0)),\n        axis.title.y = element_text(vjust = 2.5))\n\np1\n\n\n\n\nWith the overall Singapore population pyramid available, the next question to answer would be: which are the nine areas to be studied for this exercise?\nTo answer this, the population is aggregated by planning areas. Duplicate the required data and save as an individual dataset.\n\npop_temp_pa <- pop_temp %>% \n  group_by(PA) %>% \n  summarise(total_pop = sum(Pop)) %>% \n  arrange(desc(total_pop))\n\n\n\n4.5.2 Population distribution by planning areas\nPopulation of each area is displayed using bar chart. With this, the distribution of population by areas is easily visible.\n\np2 <- \nggplot(data=pop_temp_pa,\n       aes(x=reorder(PA, +total_pop),\n           y=total_pop)) + \n  geom_col() +\n  xlab(NULL) +\n  ylab(NULL) +\n  ggtitle(\"Population of Singapore by planning areas 2022\") +\n  labs(caption = \"Demographic information as of Jun 2022\") +\n  coord_flip() +\n  scale_y_continuous(breaks = seq(0, 300000, 50000), \n                      labels = paste0(as.character(c(seq(0, 300, 50))),\"k\"),\n                     expand = c(0, 0)) +\n  scale_x_discrete(expand = c(0.01, 0)) +\n  theme_economist() +\n  theme(plot.title = element_text(size = 15,\n                                  hjust = 0.5,\n                                  margin = margin(0,0,10,0)),\n        axis.text.y = element_text(size = 10))\n\np2\n\n\n\n\n\n\n4.5.3 Population pyramid of nine selected planning areas\nUsing the above chart, it is obvious that the top nine planning areas with largest population are:\n\nBedok,\nTampines,\nJurong West,\nSengkang,\nWoodlands,\nHougang,\nYishun,\nChoa Chu Kang,\nPunggol.\n\nConsidering their rather significant population numbers, further study of population pyramid would be conducted on them.\n\ntop_9 = c(\"Bedok\",\n          \"Tampines\",\n          \"Jurong West\",\n          \"Sengkang\",\n          \"Woodlands\",\n          \"Hougang\",\n          \"Yishun\",\n          \"Choa Chu Kang\",\n          \"Punggol\")\n\npop_top_9 = pop_temp %>% \n              filter(pop_temp$PA %in% top_9)\n\nBy filtering the overall Singapore data set and following the same approach on plotting the overall Singapore population pyramid, the nine chosen areas’ population pyramid are therefore presented below:\n\np3 <- \nggplot(data=pop_top_9,\n       aes(x=AG,\n           y=Pop_abs,\n           fill = Sex)) +\n  geom_bar(data=pop_top_9,stat = \"identity\") +\n  xlab(\"Age Group\") +\n  ylab(NULL) +\n  coord_flip() +\n  facet_wrap( ~ PA) + \n  scale_y_continuous(breaks = seq(-15000, 15000, 5000), \n                     labels = paste0(as.character(c(seq(15, 0, -5), seq(5, 15, 5))),\"k\")) + \n  ggtitle(\"Females vs Males population in Top 9 Population Areas of Singapore by age group 2022\") +\n  labs(caption = \"Demographic information as of Jun 2022\") +\n  scale_fill_discrete(name=\"\") +\n  theme_economist() + \n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 15,\n                                  hjust = 0.5,\n                                  margin = margin(0,0,10,0)),\n        # axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 10)),\n        axis.title.y = element_text(vjust = 2.5),\n        strip.text = element_text(vjust = 1))\n\np3\n\n\n\n\n\n\n4.5.4 Final dashboard\nTo complete the story within one dashboard, three graphs above are composed into one using Patchwork.\n\n(p1 + p2) / p3"
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html",
    "title": "Take-home Ex03",
    "section": "",
    "text": "For this assignment, the salient patterns of the resale prices of public housing properties by residential towns and estates in Singapore will be explored using dataset taken from Data.gov.sg.\nIf you would like to read the conclusion straight, please click Section 4 to reach the bottom of this study.\n\n\nFor this exercise,\n\ntidyverse is the main package to be used for data processing\nDT is the package to be used for interactive data preview\nggstatsplot is the packge to be used for statistical analysis and visualisation\nggiraph, gganimate and gifski are packages to enable interactive data visualisation\nOther packages are for make-up mainly\n\n\npacman::p_load(tidyverse,\n               DT,\n               ggstatsplot,\n               ggiraph,\n               gganimate,\n               gifski,\n               ggthemes)\n\n\n\n\n\nflat_full <- read_csv(\"Data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n\n\nDT::datatable(flat_full,\n              class =\"cell-border stripe\")\n\n\n\n\n\n\nThrough the preview of the dataset, certain data processing would be beneficial for easier analysis later on. E.g. the ‘month’ column will be split into two columns ‘year_’ and ‘month_’. Information like price per square meter and remaining number of years on the lease will also be calculated.\n\n\n\n\nflat_temp <- flat_full %>%\n  separate(month,\n           into = c(\"year_\", \"month_\"),\n           sep = \"-\",\n           convert = TRUE) %>% \n  mutate(price_psqm = resale_price / floor_area_sqm)\n\nflat_temp[\"rem_lease_yrs\"] = (99 - (2022 - flat_temp$lease_commence_date))\n\n\nDT::datatable(flat_temp,\n              class =\"cell-border stripe\")"
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html#three-sample-mean-test",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html#three-sample-mean-test",
    "title": "Take-home Ex03",
    "section": "3.1 Three-sample mean test",
    "text": "3.1 Three-sample mean test\n\nggbetweenstats(\n  data = flat_2022_3types,\n  x = flat_type,\n  y = resale_price,\n  type = \"np\",\n  plot.type = \"boxviolin\",\n  title = \"Non-parametric mean test for 3, 4 & 5-room HDB\",\n  xlab = \"Flat type\",\n  ylab =\"Resale price\") +\n  theme_economist() +\n  theme(axis.title.y = element_text(vjust = 2.5))\n\n\n\n\nFigure 2: three-sample mean test\n\n\n\n\nUsing a non-parametric test, from the result p = 0 < 0.05, it can be concluded that the resale price distribution does not follow a normal distribution."
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html#median-resale-price-trend-in-2022",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html#median-resale-price-trend-in-2022",
    "title": "Take-home Ex03",
    "section": "3.2 Median resale price trend in 2022",
    "text": "3.2 Median resale price trend in 2022\nTo look at the resale price trend in 2022, 2022 resale transaction data is grouped by flat type and month for easier visualisation.\n\nflat_by_mth <- flat_2022_3types %>% \n  group_by(flat_type, month_) %>% \n  summarise(total_sales = n(),\n            median_sales_price = median(resale_price),\n            median_house_size_sqm = median(floor_area_sqm),\n            median_remaining_lease_yrs = median(rem_lease_yrs),\n            max_sales_price = max(resale_price),\n            min_sales_price = min(resale_price)) %>% \n  arrange(desc(total_sales))\n\n\nflat_by_mth$tooltip <- c(paste0(\n  \"Flat type: \", flat_by_mth$flat_type,\n  \"\\n Median resales price: \", flat_by_mth$median_sales_price\n))\n\np2 <- ggplot(data = flat_by_mth,\n             aes(x = month_, \n                 y = median_sales_price, \n                 colour = flat_type)) +\n        geom_point_interactive(aes(tooltip = flat_by_mth$tooltip)) +\n        geom_smooth() +\n        scale_y_continuous(labels = function(x) format(x, scientific = FALSE)) +\n        scale_x_continuous(breaks = seq_along(month.abb),\n                           labels = month.abb) +\n        theme_economist() +\n        theme(axis.title.y = element_text(vjust = 2.5),\n              legend.position = \"right\") +\n  labs(x = NULL,\n       y = \"Median resale price\") +\n  ggtitle(\"Median resale price trend in 2022 for 3, 4 and 5 ROOM HDB\")\n\n\ngirafe(\n  ggobj = p2,\n  width_svg = 12\n)\n\n\n\n\nFigure 3: median resale price trend\n\n\n\nBased on the point plot, it is observed that the median resale transaction price for all three flat types has been on an increasing trend throughout the whole year of 2022. This indeed aligns with my experience as a house tenant for the year, during which most of room rental or purchase contracts have seen significant surges.\nBesides, the price increment from adding one more room is roughly $100,000."
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html#resale-price-psm-by-town-for-each-flat-type",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html#resale-price-psm-by-town-for-each-flat-type",
    "title": "Take-home Ex03",
    "section": "3.3 Resale price psm by town for each flat type",
    "text": "3.3 Resale price psm by town for each flat type\nTo visualise the price distribution by towns, a boxplot is chosen here to present summary statistics all in one panel.\n\nggplot(flat_2022_3types,\n       aes(x = town, \n           y = price_psqm,\n           color = town)) +\n  geom_boxplot() +\n  facet_wrap(~ flat_type, nrow = 3) +\n  labs(title = \"Flat Resale Price (psm) by Town for 3, 4, 5-room HDB flats\",\n       x = NULL,\n       y = \"Resale Price ($ psm)\") +\n  guides(color = FALSE) +\n  theme_economist() +\n  theme(plot.title = element_text(size=16, hjust=0.5),\n        axis.text.x = element_text(vjust = 0.5,\n                                   angle = 60,\n                                   size = 8),\n        axis.title.y = element_text(size=15,\n                                    vjust = 2.5),\n        panel.spacing = unit(3, \"line\"))\n\n\n\n\nFigure 4: resale price psm by town\n\n\n\n\nUsing the chart, the resale price psm for one town remain largely the same across three flat types.\nFor 3-room type, the median resale price psm are rather close for all towns. The maximum resale price psm for 3-room type was seen in Bukit Merah area and the minimum resale unit price was most likely seen in Toa Payoh.\nFor 4-room type, the median resale price psm in Central area and in Queenstown were rather higher than most of the rest areas.\nFor 5-room type, the median resale price psm in Central area were extremely high, while the rest of areas’ were relatively close.\nWhat’s more, the price psm for each flat type could vary widely even within the same area, as can be seen from the difference between max and min values."
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html#median-resales-price-psm-by-town-for-each-storey-range",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html#median-resales-price-psm-by-town-for-each-storey-range",
    "title": "Take-home Ex03",
    "section": "3.4 Median resales price psm by town for each storey range",
    "text": "3.4 Median resales price psm by town for each storey range\nTo visualise the potential impact from the location (town) and the storey range, a heatmap is chosen here to cross examine multivariate data. This is because heatmaps are generally good for showing variance across multiple variables to reveal patterns.\n\nflat_heatmap <- flat_2022_3types %>% \n  group_by(town, storey_range) %>% \n  summarise(total_sales = n(),\n            median_sales_price = median(resale_price),\n            median_house_size_sqm = median(floor_area_sqm),\n            median_remaining_lease_yrs = median(rem_lease_yrs)) %>% \n  arrange(desc(total_sales))\n\n\nheatmap <- ggplot(data = flat_heatmap, \n                  mapping = aes(x = town, \n                                y = storey_range,\n                                fill = median_sales_price)) +\n            geom_tile() +\n  labs(title = \"Median resales price per square meter by town and storey for 3, 4 & 5-Room HDB flat\", \n       x = NULL, \n       y = \"Storey\") +\n  scale_fill_gradient(name = \"Price per square meter\",\n                      low = \"#F2F2F2\",\n                      high = \"#00532F\")+\n  theme_economist() +\n  theme(axis.text.x = element_text(angle = 60,\n                                   vjust = 0.5,\n                                   size = 8),\n        axis.title.y = element_text(vjust = 2.5),\n        legend.position = \"right\",\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8))\n\nheatmap\n\n\n\n\nFigure 5: heatmap by town and storey range\n\n\n\n\nBased on the heatmap produced, it is generally true that higher storey units tends to have a better price psm, as illustrated from the darker green on higher storeys.\nHowever, for some unpopular towns like Choa Chu Kang, Woodlands, Jurong West, Pasir Ris, Punggol, Sembawang and Yishun, the price psm difference do not seem significant between higher and lower storey units.\nAdditionally, it can be observed that resale flats in Bukit Timah and Yishun are much lower rised compared to the rest of towns. Especially, the highest unit observed in Bukit Timah was only 15-storey. On the other hand, resales flats in Central Area are the highest with storey higher than 50 available. Besides, the price psm for 30-stroey in Central Area were mostly highest comparing to other towns."
  },
  {
    "objectID": "Take-home_Ex/Ex03/Take-home_Ex03.html#resales-price-per-psm-against-remaining-lease-years-throughout-2022",
    "href": "Take-home_Ex/Ex03/Take-home_Ex03.html#resales-price-per-psm-against-remaining-lease-years-throughout-2022",
    "title": "Take-home Ex03",
    "section": "3.5 Resales price per psm against remaining lease years throughout 2022",
    "text": "3.5 Resales price per psm against remaining lease years throughout 2022\nThe last factor that is being looked into is the the number of years remained on the lease. To isolate the potential impact from different month of the year (e.g. bonus payout, school holidays or holiday seasons), a scatter plot enabled by animation of transition of months is presented below.\n\np3 <-\nggplot(flat_2022_3types,\n       aes(x = rem_lease_yrs,\n           y = price_psqm,\n           colour = flat_type)) +\n  geom_point() +\n  labs(title = \"Resale price per square meter against remaining lease years\",\n       x = \"Remaining Lease (Years)\",\n       y = \"Resale price ($ psm)\",\n       fill =\"Flat type\",\n       caption = \"Month of 2022: {frame_time}\") +\n  theme_economist() +\n  geom_smooth(method=\"lm\",\n              se = FALSE,\n              color = \"mediumorchid1\",\n              formula = y ~ x) +\n  theme(plot.title = element_text(size = 12,\n                                  hjust = 0),\n        axis.title.x = element_text(size = 12,\n                                    vjust = 0.5),\n        axis.title.y = element_text(size = 12,\n                                    vjust = 2.5),\n        legend.position = \"none\",\n        panel.spacing = unit(2, \"line\")) +\n  facet_grid(flat_type ~ .) +\n  transition_time(flat_2022_3types$month_) +\n  ease_aes('linear')\n\nanimate(p3,\n        nframes = 12,\n        fps = 0.5)\n\n\n\n\nFigure 6: scatterplot animation by month\n\n\n\n\nAccording to the chart, throughout 2022 from Janurary to December, it’s certain that the resale price psm increases along the number of years remaining on the lease.\nWhat is also observed was that most of resale transactions for those with less than 50 years on the lease were 3-room flat mainly.\nFurthermore, relatively more transactions are seen particularly for those with the remaining lease above than 90 years."
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html",
    "title": "Take-home Ex04",
    "section": "",
    "text": "For this assignment, the impact of COVID-19 as well as the global economic and political dynamic from 2020 to 2022 on Singapore bi-lateral trade will be uncovered with visual analytics. Dataset is taken from Department of Statistics Singapore."
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html#libraries-loading",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html#libraries-loading",
    "title": "Take-home Ex04",
    "section": "2 Libraries Loading",
    "text": "2 Libraries Loading\nFor this exercise,\n\nreadxl is the package for data importing from excel files\nCPGfunctions is the package for slopegraph plotting\nggHoriPlot is the package for horizontal graph plotting\ntidyverse is the main package for data processing\nOther packages are for make-up or minor usage mainly\n\n\npacman::p_load(DT,\n               readxl,\n               CGPfunctions,\n               ggHoriPlot,\n               lubridate,\n               ggthemes,\n               tidyverse)"
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html#data-loading",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html#data-loading",
    "title": "Take-home Ex04",
    "section": "3 Data Loading",
    "text": "3 Data Loading\nTrade data downloaded from the website is in Excel spreadsheet format.\n\ntrade_export_temp <- read_excel(\"Data/Merchandise trade by region_market.xlsx\",\n                       sheet = \"T1\") \n\ntrade_import_temp <- read_excel(\"Data/Merchandise trade by region_market.xlsx\",\n                       sheet = \"T2\") \n\nTaking the first look at the data, the table is messy with rows that are not needed. The import and export data tables are separated into two tables at the moment.\n\nDT::datatable(trade_import_temp,\n              class = \"cell-border stripe\")"
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html#data-processing",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html#data-processing",
    "title": "Take-home Ex04",
    "section": "4 Data Processing",
    "text": "4 Data Processing\nWe will therefore remove those irrelevant rows and promote the “Data Series” row as our table title.\nTo consolidate the export and import tables, a column with the column name “trade” will be tagged to each row, whereby whether the row belongs to “Export” or “Import” will be categorised.\n\ncolnames(trade_import_temp) <- as.character(unlist(trade_import_temp[9,]))\ncolnames(trade_export_temp) <- as.character(unlist(trade_export_temp[9,]))\n\n\ntrade_export_temp <- trade_export_temp[c(11:(nrow(trade_export_temp)-23)),]\nexport_vector <- rep(\"Export\",\n                     nrow(trade_export_temp))\n\ntrade_export_temp$trade <- export_vector\n\ntrade_import_temp <- trade_import_temp[c(11:(nrow(trade_import_temp)-23)),]\nimport_vector <- rep(\"Import\",\n                     nrow(trade_import_temp))\n\ntrade_import_temp$trade <- import_vector\n\n\nnames(trade_export_temp)[1] <- \"Market_\"\nnames(trade_import_temp)[1] <- \"Market_\"\n\nFor this exercise, the period of January 2020 to December 2022 is to be studied.\nTherefore, only columns of months from 2020, 2021 and 2022 are identified and kept.\n\ntrade_export_unpiv <- trade_export_temp[,c(1, 3:38, ncol(trade_export_temp))]\ntrade_import_unpiv <- trade_import_temp[,c(1, 3:38, ncol(trade_import_temp))]\n\nWith that, tables of export and import data are merged together. Columns of trade data by months are gathered (pivoted) into one column. Noticed that some of data are in the unit of millions while some are in thousands. To ensure data consistency, we will make adjustments by translating all data into the unit of dollar.\n\ntrade_full <- rbind(trade_export_unpiv,\n                    trade_import_unpiv) %>% \n  gather(key = \"Month_\", value = \"Trade_Amount\", -Market_, -trade) %>% \n  mutate(month_num = ym(Month_))\n\n\ntrade_full$`Trade_Amount` <- as.numeric(trade_full$`Trade_Amount`)\n\ntrade_full$`Trade_Amount` <- ifelse(grepl(\"Thousand Dollars\", trade_full$Market_), trade_full$'Trade_Amount' * 1000, trade_full$'Trade_Amount' * 1000000)\n\ntrade_full$Market_ <- gsub(\" \\\\(Thousand Dollars\\\\)|\\\\(Million Dollars\\\\)\", \"\", trade_full$Market_)\n\nThe column of “Month_” is split into “Year” and “Month” for convenience.\n\ntrade_clean <- \nseparate(trade_full,\n         Month_,\n         into = c(\"Year\", \"Month\"),\n         sep = \" \",\n         convert = TRUE,\n         remove = TRUE)\n\ntrade_clean$Trade_Amount <- ifelse(is.na(trade_clean$Trade_Amount), 0, trade_clean$Trade_Amount)\n\n\nmonth_order <- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                 \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\ntrade_clean$Month <- factor(trade_clean$Month, levels = month_order)\n\nDT::datatable(trade_clean,\n              class = \"cell-border stripe\")\n\n\n\n\n\n\nA table of trade_clean but in wide format is also produced and saved inside the “diff” column for net export trade calculation.\n\ntrade_clean_wide <- trade_clean %>% \n  pivot_wider(names_from = trade,\n              values_from = Trade_Amount) \n\n\ntrade_clean_wide$diff = trade_clean_wide$Export - trade_clean_wide$Import"
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html#visual-analysis",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html#visual-analysis",
    "title": "Take-home Ex04",
    "section": "5 Visual Analysis",
    "text": "5 Visual Analysis\n\n5.1 Trade volume from different continents\nA slopegraph of total trade volume with Singapore from each of continents is produced below for the period of 2020 to 2022 during the pandemic.\nThe slopegraph is chosen as it is capable of revealing the trade volumes change between two time points.\n\ntrade_region <- trade_clean %>% \n  filter(trimws(Market_) %in% c(\"America\", \"Asia\", \"Europe\", \"Oceania\", \"Africa\"))\n\n\np1 <- trade_region %>% \n  group_by(Market_, Year) %>% \n  summarise(Trade_Total = round(sum(Trade_Amount) / 1000000000, 2)) %>% \n  mutate(year = factor(Year)) %>% \n  newggslopegraph(year, Trade_Total, Market_,\n                  LineThickness = 0.75,\n                  YTextSize = 4,\n                  Title = \"Trade Volume by continents (in $Billions)\",\n                  SubTitle = \"From 2020 to 2022\",\n                  Caption = \"Total trade value is the sum of export from and import to Singapore\") +\n  labs(y = \"Total trade value ($Billions)\",\n      x = \"Year\") +\n  theme_economist() +\n  theme(legend.position = \"null\",\n        axis.title.y = element_text(vjust = 2.5),\n        plot.title = element_text(vjust = 2.5))\n\np1\n\n\n\n\nFigure 1: Trade volume from five contients in a slopegraph\n\n\n\n\n\n\n5.2 Singapore’s trade surplus/deficit with the top 10 trade volume countries\nA positive balance of trade, known as a trade surplus, occurs when a country exports more goods than it imports. This usually means the country is earning more from its exports than it is spending on its imports, and it is generally seen as a sign of economic strength. On the other hand, trade deficits occurs when a country imports more goods than it exports.\nHere, we will look at the top 10 countries of trade volumes with Singapore in 2022 and their trade balance during the 3-year span. A horizontal plot is chosen here to enable readability with sufficient horizontal space.\n\ntrade_top10country_2022 <- trade_clean %>% \n  filter(!trimws(Market_) %in% c(\"America\", \"Asia\", \"Europe\", \"Oceania\", \"Africa\", \"European Union\")) %>% \n  filter(Year == 2022) %>% \n  group_by(Market_, Year) %>% \n  summarise(Trade_Total = round(sum(Trade_Amount) / 1000000000, 2)) %>% \n  arrange(desc(Trade_Total))\n\n\ntop10_2022 <- head(trade_top10country_2022$Market_, 10)\n\n\np2 <- trade_clean_wide %>% \n  filter(Market_ %in% top10_2022) %>% \n  ggplot() +\n  geom_horizon(aes(x = month_num,\n                   y = diff),\n               origin = 0,\n               horizonscale = 8) +\n  facet_grid(Market_ ~.) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2021-1-1\")),\n             color = \"black\",\n             linetype = \"dashed\",\n             linewidth = 0.5) +\n  geom_vline(xintercept = as.numeric(as.Date(\"2022-1-1\")),\n             color = \"black\",\n             linetype = \"dashed\",\n             linewidth = 0.5) +\n  theme_economist() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y = unit(0, \"lines\"),\n        strip.text.y = element_text(size = 10,\n                                    angle = 0,\n                                    hjust = 0.5),\n        legend.position = 'none',\n        axis.text.y = element_blank(),\n        axis.text.x = element_text(size = 10, \n                                    angle = 0,\n                                    hjust = 0.5),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.border = element_blank()\n        ) +\n  ggtitle('Net Export from Top 10 Total Trade Value Countries with Singapore') +\n  labs(subtitle = \"Net Export = Export - Import. Blue means Surplus, red means deficit.\")\n\np2\n\n\n\n\nFigure 2: Net export value from top 10 countries of total trade value with Singapore\n\n\n\n\n\n\n5.3 Top 10 trading countries monthly trade volume (2020 to 2022)\nWe are also interested in the trend of the trade volume by month from each of the top 10 countries, to see if there is any seasonality pattern. A cycle plot is captured to present how values vary over a period of time.\n\ntrade_cycle <- trade_clean %>% \n  filter(Market_ %in% top10_2022) %>% \n  mutate(Trade_Amount = Trade_Amount / 1000000) %>% \n  group_by(Market_, Month, Year) %>% \n  summarise(total_trade_amt = sum(Trade_Amount))\n\n\np3 <- ggplot(data = trade_cycle) +\n  geom_line(aes(x = Year,\n                y = total_trade_amt,\n                group = Market_,\n                color = Market_)) +\n  geom_text(data = . %>% \n              group_by(Market_) %>% \n              filter(Year == max(Year)),\n            aes(x = Year, \n                y = total_trade_amt, \n                label = Market_),\n            nudge_x = 0, \n            nudge_y = 0, \n            hjust = 0.95,\n            size = 3.5) +\n  scale_x_continuous(breaks = seq(2020, 2022, 1),\n                     labels = c(\"2020\", \"2021\", \"2022\")) +\n  facet_wrap(~Month,\n             ncol = 6) +\n  theme_economist() +\n    labs(title = \"Top 10 trading countries trade volume by month (2020 to 2022)\",\n         y = \"Total Trade Amount ($millions)\",\n         x = NULL) +\n    theme(plot.title = element_text(vjust = 2.5),\n          legend.position = \"none\",\n          axis.title.y = element_text(vjust = 2.5),\n          panel.spacing.x = unit(0.9, \"line\"))\np3\n\n\n\n\nFigure 3: Total trade amount by month from top 10 countries of trade value with Singapore"
  },
  {
    "objectID": "Take-home_Ex/Ex04/Take-home_Ex04.html#insights-and-thoughts",
    "href": "Take-home_Ex/Ex04/Take-home_Ex04.html#insights-and-thoughts",
    "title": "Take-home Ex04",
    "section": "6 Insights and thoughts",
    "text": "6 Insights and thoughts\ni. Singapore’s bi-lateral trade with the rest of the world is in the process of recovering from the impact of COVID-19 and global economic and political dynamic. Across the five continents, an upward trend is observed from 2020 to 2022, as seen in Figure 1\nii. The bulky portion of Singapore’s bi-lateral trade comes from Asia. According to Figure 3,\n\nMainland China is undoubtedly the biggest contributor.\nMalaysia, Taiwan, Hong Kong and Indonesia are the rest of top 5 trade partners from Asia.\n\niii. Trade deficit is constantly observed from Singapore’s partnership with Japan, Malaysia and Taiwan, while a consistent surplus is observed from the trade partnership with Australia, Hong Kong, Indonesia, using the color from Figure 2\niv. The trade volume is rather consistent throughout the year for different months, with the exception of February, in which a lower value is observed for most Asian selected countries in all three years. This could be largely due to two main reasons:\n\nThere are usually less number of days in the calendar month of February.\nMost of Chinese New Year holidays fall in the month of February, both sides’ work and operations could be in a slower pace as compared to the rest of months."
  },
  {
    "objectID": "In-class_Ex/Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/Ex03/In-class_Ex03.html",
    "title": "In-class Ex03",
    "section": "",
    "text": "1 Installing and loading R packages\nPackages that will be installed and loaded: ggiraph, plotly, gganimate, DT, tidyverse, patchwork. Noted that it is recommended to load tidyverse last so as to avoid some potential conflicting packages being used.\n\npacman::p_load(ggiraph,\n               plotly,\n               gganimate,\n               DT,\n               tidyverse,\n               patchwork)\n\n\n\n2 Importing data\n\nexam_data <- read_csv(\"Data/Exam_data.csv\")\n\n\n\n3 Tooltip effect with tooltip aesthetic (ggirafe)\nInteractivity: By hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\nBased on hands-on exercise 1, usually this is how a dot plot looks like. And this is a static plot.\n\np_old <- ggplot(data=exam_data,\n                aes(x = MATHS)) +\n  geom_dotplot(binwidth = 2.5, \n               dotsize = 0.5) +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\n\np_old\n\n\n\n\nBy using ggirafe, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nRemark: svg: Scalable vector graphics, the graph would scale accordingly on different devices to preserve the image quality.\n\n\n4 Multiple information on tooltip\nTo present more information inside tooltip, a new data column could be created in order to store the information needed.\n\nexam_data$tooltip <- c(paste0(\n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip),\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nEnd\nTesting"
  },
  {
    "objectID": "In-class_Ex/Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 04",
    "section": "",
    "text": "pacman::p_load(plotly,\n               DT, \n               patchwork,\n               tidyverse,\n               ggstatsplot,\n               readxl,\n               performance,\n               parameters,\n               see,\n               gtsummary)"
  },
  {
    "objectID": "In-class_Ex/Ex04/In-class_Ex04.html#two-sample-mean-test-using-ggbetweenstats",
    "href": "In-class_Ex/Ex04/In-class_Ex04.html#two-sample-mean-test-using-ggbetweenstats",
    "title": "In-class Exercise 04",
    "section": "2.1 Two-sample mean test using ggbetweenstats()",
    "text": "2.1 Two-sample mean test using ggbetweenstats()\n\nggbetweenstats(\n  data = exam_data,\n  x = GENDER, \n  y = MATHS,\n  type = \"p\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "In-class_Ex/Ex04/In-class_Ex04.html#build-a-visual-for-significant-test-of-correlation-using-ggscatterstats",
    "href": "In-class_Ex/Ex04/In-class_Ex04.html#build-a-visual-for-significant-test-of-correlation-using-ggscatterstats",
    "title": "In-class Exercise 04",
    "section": "2.2 Build a visual for Significant Test of Correlation using ggscatterstats()",
    "text": "2.2 Build a visual for Significant Test of Correlation using ggscatterstats()\n\nggscatterstats(\n  data = exam_data,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = TRUE,\n  )\n\n\n\n\n\ncar_resale <- read_xls(\"Data/ToyotaCorolla.xls\",\n                       \"data\")\n\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model       Price Age_0…¹ Mfg_M…² Mfg_Y…³     KM Quart…⁴ Weight Guara…⁵\n   <dbl> <chr>       <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>\n 1    81 TOYOTA Cor… 18950      25       8    2002  20019     100   1180       3\n 2     1 TOYOTA Cor… 13500      23      10    2002  46986     210   1165       3\n 3     2 TOYOTA Cor… 13750      23      10    2002  72937     210   1165       3\n 4     3  TOYOTA Co… 13950      24       9    2002  41711     210   1165       3\n 5     4 TOYOTA Cor… 14950      26       7    2002  48000     210   1165       3\n 6     5 TOYOTA Cor… 13750      30       3    2002  38500     210   1170       3\n 7     6 TOYOTA Cor… 12950      32       1    2002  61000     210   1170       3\n 8     7  TOYOTA Co… 16900      27       6    2002  94612     210   1245       3\n 9     8 TOYOTA Cor… 18600      30       3    2002  75889     210   1245       3\n10    44 TOYOTA Cor… 16950      27       6    2002 110404     234   1255       3\n# … with 1,426 more rows, 28 more variables: HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>,\n#   Radio <dbl>, Mistlamps <dbl>, Sport_Model <dbl>, Backseat_Divider <dbl>, …\n\n\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\nlm is an original R function building a linear regression model.\n\ntbl_regression(model,\n               intercept = TRUE)\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,636,783\n-3,150,331, -2,123,236\n<0.001\n    Age_08_04\n-14\n-35, 7.1\n0.2\n    Mfg_Year\n1,315\n1,059, 1,571\n<0.001\n    KM\n-0.02\n-0.03, -0.02\n<0.001\n    Weight\n19\n17, 21\n<0.001\n    Guarantee_Period\n28\n3.8, 52\n0.023\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\nmodel_n <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n <- check_normality(model_n)\nplot(check_n)\n\n\n\n\n\ncheck_h <- check_heteroscedasticity(model_n)\nplot(check_h)\n\n\n\n\n\nggcoefstats(model_n, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "",
    "text": "This hands-on exercise aims to"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#packages-installation",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#packages-installation",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "1 Packages installation",
    "text": "1 Packages installation\n\npackages = c('igraph', \n             'tidygraph', \n             'ggraph', \n             'visNetwork', \n             'lubridate', \n             'clock',\n             'tidyverse', \n             'graphlayouts')\n\nfor(p in packages){\n  if(!require(p, character.only = T)){\n    install.packages(p)\n  }\n  library(p, character.only = T)\n}"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#dataset",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#dataset",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "2 Dataset",
    "text": "2 Dataset\nThe data to be used in this exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nSome data wrangling is needed. For example, notice that the SentDate is currently in character format. We will need to change it back to date data type."
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#data-wrangling",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#data-wrangling",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "3 Data Wrangling",
    "text": "3 Data Wrangling\n\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\nBoth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times. - dmy() transforms the SentDate to Date data type. - wday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the data spelling in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field. - the values in the Weekday field are in ordinal scale.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\nA close examination of GAStech_edges data frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 10\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n$ SendDate    <date> 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0…\n$ Weekday     <ord> Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#create-network-objects-using-tidygraph",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#create-network-objects-using-tidygraph",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "4 Create network objects using tidygraph",
    "text": "4 Create network objects using tidygraph\nIn this section, we learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network.\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n.N() function is used to gain access to the node data while manipulating the edge data.\n.E() will give you the edge data.\n.G() will give you the tbl_graph object itself.\n\n\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# … with 48 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# … with 1,369 more rows\n\n\nFrom the output, notice that the node data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\nWe can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\n\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# … with 1,366 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# … with 51 more rows"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#plot-network-data-with-ggraph-package",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#plot-network-data-with-ggraph-package",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "5 Plot network data with ggraph package",
    "text": "5 Plot network data with ggraph package\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are: nodes, edges and layouts.\n\n5.1 Plot a basic network graph\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\nHere, the basic plotting function is ggraph(), which takes the data to be used for the graph and the type of layout desired. Both of the arguments for ggraph() are built around igraph. Therefore, ggraph() can use either an igraph object or a tbl_graph object.\n\n\n5.2 Change the default network graph theme\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n5.3 Change the coloring of the plot\n\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n5.4 Change the layout of ggraph\nggraph() support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl.\n\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n5.5 Change the corloring of the nodes\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nTake note that geom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n5.6 Change the edges\nWe can map the thickness of edge to the Weight variable.\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n5.7 Creating facet graphs\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#network-metrics-analysis",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "6 Network metrics analysis",
    "text": "6 Network metrics analysis\n\n6.1 Computing centrality indices\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n6.2 Visualising network metrics\n\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n6.3 Visualing community\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\n\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#interactive-network-graph",
    "href": "Hands-on_Ex/Ex08/Hands-on_Ex08.html#interactive-network-graph",
    "title": "Hands-on Ex08: Modelling, visualising and analysing network data with R",
    "section": "7 Interactive network graph",
    "text": "7 Interactive network graph\n\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n7.1 To include a layout\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n7.2 To include visual attributes - nodes\n\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n7.3 To include visual attributes - edges\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n7.4 Interactivity\nvisOptions() is used to incorporate interactivity features in the data visualisation. - The argument highlightNearest highlights nearest when clicking a node. - The argument nodesIdSelection adds an id node selection creating an HTML select element.\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Hands-on_Ex/Ex09/Hands-on_Ex09.html",
    "href": "Hands-on_Ex/Ex09/Hands-on_Ex09.html",
    "title": "Hands-on_Ex09: Information Dashboard Design using R",
    "section": "",
    "text": "pacman::p_load(lubridate, \n               ggthemes, \n               reactable,\n               reactablefmtr, \n               gt, \n               gtExtras, \n               tidyverse)\n\nTake note that - gtExtras provides some additional helper functions to assist in creating beautiful tables with gt, an R package specially designed for anyone to make wonderful-looking tables using the R programming language.\n\nreactablefmtr provides various features to streamline and enhance the styling of interactive reactable tables with easy-to-use and highly-customizable functions and themes."
  },
  {
    "objectID": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#data",
    "href": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#data",
    "title": "Hands-on_Ex09: Information Dashboard Design using R",
    "section": "2 Data",
    "text": "2 Data\n\n2.1 Dataset\nFor this exercise, a personal database in Microsoft Access mdb format called Coffee Chain will be used.\nTo import that, odbcConnectAccess() from RODBC package will be used.\nlibrary(RODBC)\ncon <- odbcConnectAccess('data/Coffee Chain.mdb')\ncoffeechain <- sqlFetch(con, 'CoffeeChain Query')\nwrite_rds(coffeechain, \"data/CoffeeChain.rds\")\nodbcClose(con)\n\n\n2.2 Data preparation\nThis step below is used if coffeechain is already available in R.\n\ncoffeechain <- read_rds(\"data/CoffeeChain.rds\")\n\nSales and Budgeted Sales data at Product level is aggregated below.\n\nproduct <- coffeechain %>%\n  group_by(`Product`) %>%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %>%\n  ungroup()"
  },
  {
    "objectID": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#bullet-chart-in-ggplot2",
    "href": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#bullet-chart-in-ggplot2",
    "title": "Hands-on_Ex09: Information Dashboard Design using R",
    "section": "3 Bullet chart in ggplot2",
    "text": "3 Bullet chart in ggplot2\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                linewidth = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#sparklines-in-ggplot2",
    "href": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#sparklines-in-ggplot2",
    "title": "Hands-on_Ex09: Information Dashboard Design using R",
    "section": "4 Sparklines in ggplot2",
    "text": "4 Sparklines in ggplot2\n\nsales_report <- coffeechain %>%\n  filter(Date >= \"2013-01-01\") %>%\n  mutate(Month = month(Date)) %>%\n  group_by(Month, Product) %>%\n  summarise(Sales = sum(Sales)) %>%\n  ungroup() %>%\n  select(Month, Product, Sales)\n\nMinimum, maximum, 25th & 75th percentiles are prepared below. End of the month sales are also prepared.\n\nmins <- group_by(sales_report, Product) %>% \n  slice(which.min(Sales))\nmaxs <- group_by(sales_report, Product) %>% \n  slice(which.max(Sales))\nends <- group_by(sales_report, Product) %>% \n  filter(Month == max(Month))\n\n\nquarts <- sales_report %>%\n  group_by(Product) %>%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %>%\n  right_join(sales_report)\n\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#static-information-dashboard-design-using-gt-and-gtextras",
    "href": "Hands-on_Ex/Ex09/Hands-on_Ex09.html#static-information-dashboard-design-using-gt-and-gtextras",
    "title": "Hands-on_Ex09: Information Dashboard Design using R",
    "section": "5 Static information dashboard design using gt and gtExtras",
    "text": "5 Static information dashboard design using gt and gtExtras\n\n5.1 Simeple bullet chart\n\nproduct %>%\n  gt::gt() %>%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %>%\n  gt_theme_538()\n\n\n\n\n\n  \n  \n    \n      Product\n      current\n    \n  \n  \n    Amaretto\n          \n    Caffe Latte\n          \n    Caffe Mocha\n          \n    Chamomile\n          \n    Colombian\n          \n    Darjeeling\n          \n    Decaf Espresso\n          \n    Decaf Irish Cream\n          \n    Earl Grey\n          \n    Green Tea\n          \n    Lemon\n          \n    Mint\n          \n    Regular Espresso\n          \n  \n  \n  \n\n\n\n\n\n\n5.2 Sparklines using gtExtras\n\nreport <- coffeechain %>%\n  mutate(Year = year(Date)) %>%\n  filter(Year == \"2013\") %>%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %>%\n  group_by(Product, Month) %>%\n  summarise(Sales = sum(Sales)) %>%\n  ungroup()\n\nTake note that one of the requirements of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\nreport %>%\n  group_by(Product) %>%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   <chr>             <list>         \n 1 Amaretto          <dbl [12]>     \n 2 Caffe Latte       <dbl [12]>     \n 3 Caffe Mocha       <dbl [12]>     \n 4 Chamomile         <dbl [12]>     \n 5 Colombian         <dbl [12]>     \n 6 Darjeeling        <dbl [12]>     \n 7 Decaf Espresso    <dbl [12]>     \n 8 Decaf Irish Cream <dbl [12]>     \n 9 Earl Grey         <dbl [12]>     \n10 Green Tea         <dbl [12]>     \n11 Lemon             <dbl [12]>     \n12 Mint              <dbl [12]>     \n13 Regular Espresso  <dbl [12]>     \n\n\n\nreport %>%\n  group_by(Product) %>%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %>%\n   gt() %>%\n   gt_plt_sparkline('Monthly Sales')\n\n\n\n\n\n  \n  \n    \n      Product\n      Monthly Sales\n    \n  \n  \n    Amaretto\n          1.2K\n    Caffe Latte\n          1.5K\n    Caffe Mocha\n          3.7K\n    Chamomile\n          3.3K\n    Colombian\n          5.5K\n    Darjeeling\n          3.0K\n    Decaf Espresso\n          3.2K\n    Decaf Irish Cream\n          2.7K\n    Earl Grey\n          3.0K\n    Green Tea\n          1.5K\n    Lemon\n          4.4K\n    Mint\n          1.5K\n    Regular Espresso\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n5.3 To add statistics elements\n\nreport %>% \n  group_by(Product) %>% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %>%\n  gt() %>%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n    \n  \n  \n    Amaretto\n1016\n1210\n1,119.00\n    Caffe Latte\n1398\n1653\n1,528.33\n    Caffe Mocha\n3322\n3828\n3,613.92\n    Chamomile\n2967\n3395\n3,217.42\n    Colombian\n5132\n5961\n5,457.25\n    Darjeeling\n2926\n3281\n3,112.67\n    Decaf Espresso\n3181\n3493\n3,326.83\n    Decaf Irish Cream\n2463\n2901\n2,648.25\n    Earl Grey\n2730\n3005\n2,841.83\n    Green Tea\n1339\n1476\n1,398.75\n    Lemon\n3851\n4418\n4,080.83\n    Mint\n1388\n1669\n1,519.17\n    Regular Espresso\n890\n1218\n1,023.42\n  \n  \n  \n\n\n\n\n\n\n5.4 To combine the data frame\n\nspark <- report %>%\n  group_by(Product) %>%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\nsales <- report %>% \n  group_by(Product) %>% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\nsales_data = left_join(sales, spark)\n\n\n\n5.5 Plot the updated data table\n\nsales_data %>%\n  gt() %>%\n  gt_plt_sparkline('Monthly Sales')\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n    Colombian\n5132\n5961\n5457.250\n          5.5K\n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n    Lemon\n3851\n4418\n4080.833\n          4.4K\n    Mint\n1388\n1669\n1519.167\n          1.5K\n    Regular Espresso\n890\n1218\n1023.417\n          1.1K\n  \n  \n  \n\n\n\n\n\n\n5.6 To combine with bullet chart and sparklines\n\nbullet <- coffeechain %>%\n  filter(Date >= \"2013-01-01\") %>%\n  group_by(`Product`) %>%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %>%\n  ungroup() \n\nsales_data = sales_data %>%\n  left_join(bullet)\n\nsales_data %>%\n  gt() %>%\n  gt_plt_sparkline('Monthly Sales') %>%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %>%\n  gt_theme_538()\n\n\n\n\n\n  \n  \n    \n      Product\n      Min\n      Max\n      Average\n      Monthly Sales\n      Actual\n    \n  \n  \n    Amaretto\n1016\n1210\n1119.000\n          1.2K\n          \n    Caffe Latte\n1398\n1653\n1528.333\n          1.5K\n          \n    Caffe Mocha\n3322\n3828\n3613.917\n          3.7K\n          \n    Chamomile\n2967\n3395\n3217.417\n          3.3K\n          \n    Colombian\n5132\n5961\n5457.250\n          5.5K\n          \n    Darjeeling\n2926\n3281\n3112.667\n          3.0K\n          \n    Decaf Espresso\n3181\n3493\n3326.833\n          3.2K\n          \n    Decaf Irish Cream\n2463\n2901\n2648.250\n          2.7K\n          \n    Earl Grey\n2730\n3005\n2841.833\n          3.0K\n          \n    Green Tea\n1339\n1476\n1398.750\n          1.5K\n          \n    Lemon\n3851\n4418\n4080.833\n          4.4K\n          \n    Mint\n1388\n1669\n1519.167\n          1.5K\n          \n    Regular Espresso\n890\n1218\n1023.417\n          1.1K"
  }
]